{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    return accuracy_score(df['label'],df['preds'])\n",
    "\n",
    "def miss(df):\n",
    "    return df[df['label']!=df['preds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['locationstest_babe_base.csv',\n",
       " 'factualtest_magpie.csv',\n",
       " 'prejudicetest_babe_base.csv',\n",
       " 'locationstest_magpie.csv',\n",
       " 'factualtest_roberta_anno_lexical.csv',\n",
       " 'factualtest_babe_base.csv',\n",
       " 'loadedtest_roberta_anno_lexical.csv',\n",
       " 'pronounstest_babe_base.csv',\n",
       " 'prejudicetest_magpie.csv',\n",
       " 'pronounstest_roberta_anno_lexical.csv',\n",
       " 'prejudicetest_roberta_anno_lexical.csv',\n",
       " 'locationstest_roberta_anno_lexical.csv',\n",
       " 'pronounstest_magpie.csv',\n",
       " 'loadedtest_magpie.csv',\n",
       " 'loadedtest_babe_base.csv']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFT: factual-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we took two datasets of very simple, short, factual sentences and we want to test if our model can always predict them as unbiased. An example of a sentence from the dataset: \"Food cravings tell us that we want to eat a certain food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/factualtest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/factualtest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/factualtest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9506953223767383\n",
      "0.9149810366624526\n",
      "0.9639696586599241\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nothing in particular happens if you give a man enough rope',\n",
       " 'People would disagree on the most interesting man in the world',\n",
       " 'If a lawsuit is frivolous, it lacks a legal argument',\n",
       " 'What do we call the good guy in a movie? The good guy in a movie is called the hero.',\n",
       " 'What happens to Pinocchio when he tells a lie? His nose grows longer.',\n",
       " 'Nothing happens if a shoemaker leaves unfinished shoes out overnight',\n",
       " 'You should avoid putting anything in their mouth',\n",
       " \"What is responsibility? Doing what you're supposed to do and taking care of things that are important\",\n",
       " 'There is no place that has rain literally all the time',\n",
       " 'Yes, duck quacks echo']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of 10 sentences where babe classifier wrongly predicts biased but synthetic one doesnt\n",
    "miss(babe)[~miss(babe)['text'].isin(miss(synth)['text'])].text.sample(10).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIR: loaded-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test, we take unbiased sentences from the BABE test set and inject biased adjectives and adverbs into them, artifically changing the label to biased. The model succeeds if it can change it's prediction to biased. The biased adjectives and adverbs are taken from the bias lexicon. We only take neutral sentences that the model was able to classify as neutral in the first place, to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/loadedtest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/loadedtest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/loadedtest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe = babe[babe['text_orig']!=babe['text_loaded']]\n",
    "magpie = magpie[magpie['text_orig']!=magpie['text_loaded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41604010025062654\n",
      "0.6876923076923077\n",
      "0.6651595744680852\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INV: locations-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/locationstest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/locationstest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/locationstest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9823788546255506\n",
      "0.9856502242152466\n",
      "0.9749445676274945\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INV: pronouns-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/pronounstest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/pronounstest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/pronounstest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9711884753901561\n",
      "0.9782608695652174\n",
      "0.9566787003610109\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They has been teaching a course on terrorism and insurgency at it in Pittsburgh for four years, and much more of his class these days is devoted to white supremacy than in the past.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss(magpie).text_ner_free.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Colin P. Clarke has been teaching a course on terrorism and insurgency at Carnegie Mellon University in Pittsburgh for four years, and much more of his class these days is devoted to white supremacy than in the past.'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss(magpie).text_orig.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INV: prejudice-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/prejudicetest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/prejudicetest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/prejudicetest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe = babe[babe.label==0]\n",
    "synth = synth[synth.label==0]\n",
    "magpie = magpie[magpie.label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8947368421052632\n",
      "0.891578947368421\n",
      "0.8526315789473684\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politician\n",
      "babe 1.0\n",
      "magpie 1.0\n",
      "synth 1.0\n",
      "Gender\n",
      "babe 0.7133333333333334\n",
      "magpie 0.6866666666666666\n",
      "synth 0.5733333333333334\n",
      "Political Affiliation\n",
      "babe 0.6866666666666666\n",
      "magpie 0.76\n",
      "synth 0.8533333333333334\n",
      "Occupation\n",
      "babe 1.0\n",
      "magpie 0.9866666666666667\n",
      "synth 1.0\n",
      "Disability\n",
      "babe 1.0\n",
      "magpie 0.98\n",
      "synth 1.0\n",
      "Religion\n",
      "babe 0.9333333333333333\n",
      "magpie 0.8866666666666667\n",
      "synth 0.64\n",
      "Origin\n",
      "babe 1.0\n",
      "magpie 1.0\n",
      "synth 1.0\n"
     ]
    }
   ],
   "source": [
    "for category in babe.category.unique():\n",
    "    print(category)\n",
    "    print(f\"babe {accuracy(babe[babe.category==category])}\")\n",
    "    print(f\"magpie {accuracy(magpie[magpie.category==category])}\")\n",
    "    print(f\"synth {accuracy(synth[synth.category==category])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
