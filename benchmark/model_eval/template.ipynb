{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Eval the results of TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, \\\n",
    "    accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load\n",
    "model = 'flan-ul2'\n",
    "\n",
    "# load results\n",
    "zero_shot = pd.read_csv(f\"../zero-shot/data/{model}.csv\")\n",
    "zero_shot_with_system = pd.read_csv(\n",
    "    f\"../zero-shot-system_prompt/data/{model}.csv\")\n",
    "zero_shot_cot = pd.read_csv(f\"../zero-shot-cot/data/{model}.csv\")\n",
    "two_shot = pd.read_csv(f\"../2-shot/data/{model}.csv\")\n",
    "four_shot = pd.read_csv(f\"../4-shot/data/{model}.csv\")\n",
    "eight_shot = pd.read_csv(f\"../8-shot/data/{model}.csv\")\n",
    "\n",
    "two_shot_cot = pd.read_csv(f\"../2-shot-CoT/data/{model}.csv\")\n",
    "four_shot_cot = pd.read_csv(f\"../4-shot-CoT/data/{model}.csv\")\n",
    "eight_shot_cot = pd.read_csv(f\"../8-shot-CoT/data/{model}.csv\")\n",
    "\n",
    "#load pool\n",
    "pool = load_dataset('mediabiasgroup/BABE-icl-pool')['train'].to_pandas()\n",
    "\n",
    "# exclude pool from model (if needed)\n",
    "zero_shot = zero_shot.merge(pool['text'], on='text', how='left',\n",
    "                            indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "zero_shot_with_system = zero_shot_with_system.merge(pool['text'], on='text',\n",
    "                                                    how='left',\n",
    "                                                    indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "zero_shot_cot = zero_shot_cot.merge(pool['text'], on='text', how='left',\n",
    "                                    indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "two_shot = two_shot.merge(pool['text'], on='text', how='left',\n",
    "                          indicator=True).query('_merge == \"left_only\"').drop(\n",
    "    '_merge', axis=1)\n",
    "four_shot = four_shot.merge(pool['text'], on='text', how='left',\n",
    "                            indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "eight_shot = eight_shot.merge(pool['text'], on='text', how='left',\n",
    "                              indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "two_shot_cot = two_shot_cot.merge(pool['text'], on='text', how='left',\n",
    "                              indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "four_shot_cot = four_shot_cot.merge(pool['text'], on='text', how='left',\n",
    "                              indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "eight_shot_cot = eight_shot_cot.merge(pool['text'], on='text', how='left',\n",
    "                              indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "\n",
    "\n",
    "#load babe\n",
    "dataset = load_dataset('mediabiasgroup/BABE-v4')\n",
    "df_babe = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# df_merge = babe at begin\n",
    "df_merge_all = df_babe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zero_shot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mzero_shot\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zero_shot' is not defined"
     ]
    }
   ],
   "source": [
    "zero_shot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot = zero_shot.rename(columns={\"label\": \"0_shot_label\"})\n",
    "zero_shot['0_shot_label'] = zero_shot['0_shot_label'].replace('BIASED', 1)\n",
    "zero_shot['0_shot_label'] = zero_shot['0_shot_label'].replace('NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_babe, zero_shot[['text', '0_shot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "zero_shot_label = df_merge['0_shot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO: \", f1_score(ground_truth, zero_shot_label))\n",
    "print(\"Precision with TODO: \",\n",
    "      precision_score(ground_truth, zero_shot_label))\n",
    "print(\"Recall with TODO: \",\n",
    "      recall_score(ground_truth, zero_shot_label))\n",
    "print(\"Accuracy with TODO: \",\n",
    "      accuracy_score(ground_truth, zero_shot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# zero shot with system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot_with_system.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot_with_system = zero_shot_with_system.rename(\n",
    "    columns={\"label\": \"0_shot_with_system_label\"})\n",
    "zero_shot_with_system['0_shot_with_system_label'] = zero_shot_with_system[\n",
    "    '0_shot_with_system_label'].replace('BIASED', 1)\n",
    "zero_shot_with_system['0_shot_with_system_label'] = zero_shot_with_system[\n",
    "    '0_shot_with_system_label'].replace('NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, zero_shot_with_system[\n",
    "    ['text', '0_shot_with_system_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "zero_shot_with_system_label = df_merge['0_shot_with_system_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with System Prompt: \",\n",
    "      f1_score(ground_truth, zero_shot_with_system_label))\n",
    "print(\"Precision with TODO with System Prompt: \",\n",
    "      precision_score(ground_truth, zero_shot_with_system_label))\n",
    "print(\"Recall with TODO with System Prompt: \",\n",
    "      recall_score(ground_truth, zero_shot_with_system_label))\n",
    "print(\"Accuracy with TODO with System Prompt: \",\n",
    "      accuracy_score(ground_truth, zero_shot_with_system_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# zero shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot_cot = zero_shot_cot.rename(columns={\"label\": \"0_shot_cot_label\"})\n",
    "zero_shot_cot['0_shot_cot_label'] = zero_shot_cot['0_shot_cot_label'].replace(\n",
    "    'BIASED', 1)\n",
    "zero_shot_cot['0_shot_cot_label'] = zero_shot_cot['0_shot_cot_label'].replace(\n",
    "    'NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, zero_shot_cot[['text', '0_shot_cot_label']],\n",
    "                    on='text')\n",
    "ground_truth = df_merge['label']\n",
    "zero_shot_cot_label = df_merge['0_shot_cot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with CoT: \",\n",
    "      f1_score(ground_truth, zero_shot_cot_label))\n",
    "print(\"Precision with TODO with CoT: \",\n",
    "      precision_score(ground_truth, zero_shot_cot_label))\n",
    "print(\"Recall with TODO with CoT: \",\n",
    "      recall_score(ground_truth, zero_shot_cot_label))\n",
    "print(\"Accuracy with TODO with CoT: \",\n",
    "      accuracy_score(ground_truth, zero_shot_cot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_shot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'two_shot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m two_shot \u001b[38;5;241m=\u001b[39m \u001b[43mtwo_shot\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2_shot_label\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      2\u001b[0m two_shot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_shot_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m two_shot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_shot_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIASED\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m two_shot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_shot_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m two_shot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_shot_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOT BIASED\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'two_shot' is not defined"
     ]
    }
   ],
   "source": [
    "two_shot = two_shot.rename(columns={\"label\": \"2_shot_label\"})\n",
    "two_shot['2_shot_label'] = two_shot['2_shot_label'].replace('BIASED', 1)\n",
    "two_shot['2_shot_label'] = two_shot['2_shot_label'].replace('NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, two_shot[['text', '2_shot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "two_shot_label = df_merge['2_shot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (2 shot): \",\n",
    "      f1_score(ground_truth, two_shot_label))\n",
    "print(\"Precision with TODO with (2 shot): \",\n",
    "      precision_score(ground_truth, two_shot_label))\n",
    "print(\"Recall with TODO with (2 shot): \",\n",
    "      recall_score(ground_truth, two_shot_label))\n",
    "print(\"Accuracy with TODO with (2 shot): \",\n",
    "      accuracy_score(ground_truth, two_shot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "four_shot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "four_shot = four_shot.rename(columns={\"label\": \"4_shot_label\"})\n",
    "four_shot['4_shot_label'] = four_shot['4_shot_label'].replace('BIASED', 1)\n",
    "four_shot['4_shot_label'] = four_shot['4_shot_label'].replace('NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, four_shot[['text', '4_shot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "four_shot_label = df_merge['4_shot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (4 shot): \",\n",
    "      f1_score(ground_truth, four_shot_label))\n",
    "print(\"Precision with TODO with (4 shot): \",\n",
    "      precision_score(ground_truth, four_shot_label))\n",
    "print(\"Recall with TODO with (4 shot): \",\n",
    "      recall_score(ground_truth, four_shot_label))\n",
    "print(\"Accuracy with TODO with (4 shot): \",\n",
    "      accuracy_score(ground_truth, four_shot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 8-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eight_shot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eight_shot = eight_shot.rename(columns={\"label\": \"8_shot_label\"})\n",
    "eight_shot['8_shot_label'] = eight_shot['8_shot_label'].replace('BIASED', 1)\n",
    "eight_shot['8_shot_label'] = eight_shot['8_shot_label'].replace('NOT BIASED',\n",
    "                                                                0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, eight_shot[['text', '8_shot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "eight_shot_label = df_merge['8_shot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (8 shot): \",\n",
    "      f1_score(ground_truth, eight_shot_label))\n",
    "print(\"Precision with TODO with (8 shot): \",\n",
    "      precision_score(ground_truth, eight_shot_label))\n",
    "print(\"Recall with TODO with (8 shot): \",\n",
    "      recall_score(ground_truth, eight_shot_label))\n",
    "print(\"Accuracy with TODO with (8 shot): \",\n",
    "      accuracy_score(ground_truth, eight_shot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_shot_cot = two_shot_cot.rename(columns={\"label\": \"2_shot_cot_label\"})\n",
    "two_shot_cot['2_shot_cot_label'] = two_shot_cot['2_shot_cot_label'].replace('BIASED', 1)\n",
    "two_shot_cot['2_shot_cot_label'] = two_shot_cot['2_shot_cot_label'].replace('NOT BIASED',\n",
    "                                                                0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, two_shot_cot[['text', '2_shot_cot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "two_shot_cot_label = df_merge['2_shot_cot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (2 shot CoT): \",\n",
    "      f1_score(ground_truth, two_shot_cot_label))\n",
    "print(\"Precision with TODO with (2 shot CoT): \",\n",
    "      precision_score(ground_truth, two_shot_cot_label))\n",
    "print(\"Recall with TODO with (2 shot CoT): \",\n",
    "      recall_score(ground_truth, two_shot_cot_label))\n",
    "print(\"Accuracy with TODO with (2 shot CoT): \",\n",
    "      accuracy_score(ground_truth, two_shot_cot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "four_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "four_shot_cot = four_shot_cot.rename(columns={\"label\": \"4_shot_cot_label\"})\n",
    "four_shot_cot['4_shot_cot_label'] = four_shot_cot['4_shot_cot_label'].replace('BIASED', 1)\n",
    "four_shot_cot['4_shot_cot_label'] = four_shot_cot['4_shot_cot_label'].replace('NOT BIASED',\n",
    "                                                                0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, four_shot_cot[['text', '4_shot_cot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "four_shot_cot_label = df_merge['4_shot_cot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (4 shot CoT): \",\n",
    "      f1_score(ground_truth, four_shot_cot_label))\n",
    "print(\"Precision with TODO with (4 shot CoT): \",\n",
    "      precision_score(ground_truth, four_shot_cot_label))\n",
    "print(\"Recall with TODO with (4 shot CoT): \",\n",
    "      recall_score(ground_truth, four_shot_cot_label))\n",
    "print(\"Accuracy with TODO with (4 shot CoT): \",\n",
    "      accuracy_score(ground_truth, four_shot_cot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 8-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eight_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eight_shot_cot = eight_shot_cot.rename(columns={\"label\": \"8_shot_cot_label\"})\n",
    "eight_shot_cot['8_shot_cot_label'] = eight_shot_cot['8_shot_cot_label'].replace('BIASED', 1)\n",
    "eight_shot_cot['8_shot_cot_label'] = eight_shot_cot['8_shot_cot_label'].replace('NOT BIASED',\n",
    "                                                                0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, eight_shot_cot[['text', '8_shot_cot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "eight_shot_cot_label = df_merge['8_shot_cot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (8 shot CoT): \",\n",
    "      f1_score(ground_truth, eight_shot_cot_label))\n",
    "print(\"Precision with TODO with (8 shot CoT): \",\n",
    "      precision_score(ground_truth, eight_shot_cot_label))\n",
    "print(\"Recall with TODO with (8 shot CoT): \",\n",
    "      recall_score(ground_truth, eight_shot_cot_label))\n",
    "print(\"Accuracy with TODO with (8 shot CoT): \",\n",
    "      accuracy_score(ground_truth, eight_shot_cot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Comparison and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(ax, df, true_labels_column, predicted_labels_column,\n",
    "                          title=None\n",
    "                          ):\n",
    "    predicted_labels = df[f'{predicted_labels_column}']\n",
    "    true_labels = df[f'{true_labels_column}']\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "\n",
    "    # Display confusion matrix heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True,\n",
    "                yticklabels=True, ax=ax)\n",
    "\n",
    "    title = title if title else predicted_labels_column\n",
    "\n",
    "    ax.set_title(f'Confusion Matrix - {title}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices')\n",
    "\n",
    "# Plot each confusion matrix\n",
    "plot_confusion_matrix(axes[0, 0], df_merge, 'label', '0_shot_label', '0_shot')\n",
    "plot_confusion_matrix(axes[0, 1], df_merge, 'label',\n",
    "                      '0_shot_with_system_label', '0_shot_with_system')\n",
    "plot_confusion_matrix(axes[0, 2], df_merge, 'label', '0_shot_cot_label',\n",
    "                      '0_shot_cot')\n",
    "plot_confusion_matrix(axes[1, 0], df_merge, 'label', '2_shot_label', '2_shot')\n",
    "plot_confusion_matrix(axes[1, 1], df_merge, 'label', '4_shot_label', '4_shot')\n",
    "plot_confusion_matrix(axes[1, 2], df_merge, 'label', '8_shot_label', '8_shot')\n",
    "\n",
    "plt.tight_layout(\n",
    "    rect=[0, 0, 1, 0.96])  # Adjust layout to prevent title overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Krippendorff Alpha in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "\n",
    "runs = ['0_shot_label', '0_shot_with_system_label', '0_shot_cot_label',\n",
    "        '2_shot_label', '4_shot_label', '8_shot_label']\n",
    "\n",
    "\n",
    "def compute_krippendorff_alpha(df, predicted_columns):\n",
    "    pred_map = {}\n",
    "    for run in predicted_columns:\n",
    "        predicted_labels = df[run]\n",
    "        pred_map[run] = predicted_labels\n",
    "\n",
    "    # Check if there is variability in the ratings\n",
    "    unique_labels_counts = df[predicted_columns].nunique(axis=1)\n",
    "    if unique_labels_counts.max() == 1:\n",
    "        # All ratings are the same, return a special value or handle accordingly\n",
    "        return 0\n",
    "\n",
    "    reliability_data = df[predicted_columns].values.tolist()\n",
    "\n",
    "    # Calculate Krippendorff's alpha\n",
    "    alpha = krippendorff.alpha(reliability_data=list(pred_map.values()),\n",
    "                               level_of_measurement='nominal')\n",
    "\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_value = compute_krippendorff_alpha(df_merge, runs)\n",
    "print(f\"Krippendorff's Alpha (all runs): {alpha_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def compute_krippendorff_alpha_for_k_runs(df, runs, k=None):\n",
    "    # Initialize variables to store the best combination and alpha\n",
    "    if k is None:\n",
    "        k = len(runs)\n",
    "\n",
    "    best_combination = None\n",
    "    best_alpha = 0  # Assuming alpha ranges from 0 to 1\n",
    "\n",
    "    # Iterate through all possible combinations\n",
    "    for combination in itertools.combinations(runs, k):\n",
    "\n",
    "        alpha_value = compute_krippendorff_alpha(df, list(combination))\n",
    "\n",
    "        # Print alpha for the current combination\n",
    "        print(f\"Combination: {combination}, Alpha: {alpha_value}\")\n",
    "\n",
    "        # Update best combination and alpha if a higher alpha is found\n",
    "        if alpha_value > best_alpha:\n",
    "            best_alpha = alpha_value\n",
    "            best_combination = combination\n",
    "\n",
    "    # Print the best combination and alpha\n",
    "    print(f\"\\nBest Combination: {best_combination}, Best Alpha: {best_alpha}\")\n",
    "    return best_alpha, best_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_krippendorff_alpha_for_k_runs(df_merge, runs, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a80f9e0de999ee68a5c4d727299d170641bcafce6104c26cbe6897ebd010016"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
