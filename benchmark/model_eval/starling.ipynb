{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Eval the results of TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, \\\n",
    "    accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load\n",
    "model = 'Starling-LM-7B-alpha'\n",
    "\n",
    "# load results\n",
    "#zero_shot = pd.read_csv(f\"../zero-shot/data/{model}.csv\")\n",
    "#zero_shot_with_system = pd.read_csv(\n",
    "#    f\"../zero-shot-system_prompt/data/{model}.csv\")\n",
    "#zero_shot_cot = pd.read_csv(f\"../zero-shot-cot/data/{model}.csv\")\n",
    "#two_shot = pd.read_csv(f\"../2-shot/data/{model}.csv\")\n",
    "#four_shot = pd.read_csv(f\"../4-shot/data/{model}.csv\")\n",
    "#eight_shot = pd.read_csv(f\"../8-shot/data/{model}.csv\")\n",
    "#\n",
    "#two_shot_cot = pd.read_csv(f\"../2-shot-CoT/data/{model}.csv\")\n",
    "#four_shot_cot = pd.read_csv(f\"../4-shot-CoT/data/{model}.csv\")\n",
    "eight_shot_cot = pd.read_csv(f\"../8-shot-CoT/data/{model}.csv\")\n",
    "\n",
    "#load pool\n",
    "pool = load_dataset('mediabiasgroup/BABE-icl-pool')['train'].to_pandas()\n",
    "\n",
    "# exclude pool from model (if needed)\n",
    "#zero_shot = zero_shot.merge(pool['text'], on='text', how='left',\n",
    "#                            indicator=True).query(\n",
    "#    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "#zero_shot_with_system = zero_shot_with_system.merge(pool['text'], on='text',\n",
    "#                                                    how='left',\n",
    "#                                                    indicator=True).query(\n",
    "#    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "#zero_shot_cot = zero_shot_cot.merge(pool['text'], on='text', how='left',\n",
    "#                                    indicator=True).query(\n",
    "#    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "#two_shot = two_shot.merge(pool['text'], on='text', how='left',\n",
    "#                          indicator=True).query('_merge == \"left_only\"').drop(\n",
    "#    '_merge', axis=1)\n",
    "#four_shot = four_shot.merge(pool['text'], on='text', how='left',\n",
    "#                            indicator=True).query(\n",
    "#    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "#eight_shot = eight_shot.merge(pool['text'], on='text', how='left',\n",
    "#                              indicator=True).query(\n",
    "#    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "#two_shot_cot = two_shot_cot.merge(pool['text'], on='text', how='left',\n",
    "#                              indicator=True).query(\n",
    "#    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "#four_shot_cot = four_shot_cot.merge(pool['text'], on='text', how='left',\n",
    "#                              indicator=True).query(\n",
    "#    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "eight_shot_cot = eight_shot_cot.merge(pool['text'], on='text', how='left',\n",
    "                              indicator=True).query(\n",
    "    '_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "\n",
    "\n",
    "#load babe\n",
    "dataset = load_dataset('mediabiasgroup/BABE-v4')\n",
    "df_babe = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# df_merge = babe at begin\n",
    "df_merge_all = df_babe\n",
    "\n",
    "def find_first_occurrence(string, instruction=\"Instruction:\",\n",
    "                          statement1=\"The answer is BIASED\",\n",
    "                          statement2=\"The answer is NOT BIASED\"\n",
    "                          ):\n",
    "    index_instruction = string.find(instruction)\n",
    "    index1 = string.find(statement1)\n",
    "    index2 = string.find(statement2)\n",
    "\n",
    "    if index1 == -1 and index2 == -1:\n",
    "        return \"Neither instruction nor statements found in the given string.\"\n",
    "    elif index1 == -1 and index2 == -1:\n",
    "        return \"Neither statement found in the given string.\"\n",
    "    elif index1 == -1:\n",
    "        if index_instruction == -1 or index2 < index_instruction:\n",
    "            return f\"{statement2}\"\n",
    "        else:\n",
    "            return \"After Instruction\"\n",
    "    elif index2 == -1:\n",
    "        if index_instruction == -1 or index1 < index_instruction:\n",
    "            return f\"{statement1}\"\n",
    "        else:\n",
    "            return \"After Instruction\"\n",
    "    elif index1 < index2:\n",
    "        if index_instruction == -1 or index1 < index_instruction:\n",
    "            return f\"{statement1}\"\n",
    "        else:\n",
    "            return \"After Instruction\"\n",
    "    else:\n",
    "        if index_instruction == -1 or index2 < index_instruction:\n",
    "            return f\"{statement2}\"\n",
    "        else:\n",
    "            return \"After Instruction\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zero_shot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mzero_shot\u001B[49m\u001B[38;5;241m.\u001B[39mquery(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel == \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'zero_shot' is not defined"
     ]
    }
   ],
   "source": [
    "zero_shot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot = zero_shot.rename(columns={\"label\": \"0_shot_label\"})\n",
    "zero_shot['0_shot_label'] = zero_shot['0_shot_label'].replace('BIASED', 1)\n",
    "zero_shot['0_shot_label'] = zero_shot['0_shot_label'].replace('NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_babe, zero_shot[['text', '0_shot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "zero_shot_label = df_merge['0_shot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO: \", f1_score(ground_truth, zero_shot_label))\n",
    "print(\"Precision with TODO: \",\n",
    "      precision_score(ground_truth, zero_shot_label))\n",
    "print(\"Recall with TODO: \",\n",
    "      recall_score(ground_truth, zero_shot_label))\n",
    "print(\"Accuracy with TODO: \",\n",
    "      accuracy_score(ground_truth, zero_shot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# zero shot with system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot_with_system.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot_with_system = zero_shot_with_system.rename(\n",
    "    columns={\"label\": \"0_shot_with_system_label\"})\n",
    "zero_shot_with_system['0_shot_with_system_label'] = zero_shot_with_system[\n",
    "    '0_shot_with_system_label'].replace('BIASED', 1)\n",
    "zero_shot_with_system['0_shot_with_system_label'] = zero_shot_with_system[\n",
    "    '0_shot_with_system_label'].replace('NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, zero_shot_with_system[\n",
    "    ['text', '0_shot_with_system_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "zero_shot_with_system_label = df_merge['0_shot_with_system_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with System Prompt: \",\n",
    "      f1_score(ground_truth, zero_shot_with_system_label))\n",
    "print(\"Precision with TODO with System Prompt: \",\n",
    "      precision_score(ground_truth, zero_shot_with_system_label))\n",
    "print(\"Recall with TODO with System Prompt: \",\n",
    "      recall_score(ground_truth, zero_shot_with_system_label))\n",
    "print(\"Accuracy with TODO with System Prompt: \",\n",
    "      accuracy_score(ground_truth, zero_shot_with_system_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# zero shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_shot_cot = zero_shot_cot.rename(columns={\"label\": \"0_shot_cot_label\"})\n",
    "zero_shot_cot['0_shot_cot_label'] = zero_shot_cot['0_shot_cot_label'].replace(\n",
    "    'BIASED', 1)\n",
    "zero_shot_cot['0_shot_cot_label'] = zero_shot_cot['0_shot_cot_label'].replace(\n",
    "    'NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, zero_shot_cot[['text', '0_shot_cot_label']],\n",
    "                    on='text')\n",
    "ground_truth = df_merge['label']\n",
    "zero_shot_cot_label = df_merge['0_shot_cot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with CoT: \",\n",
    "      f1_score(ground_truth, zero_shot_cot_label))\n",
    "print(\"Precision with TODO with CoT: \",\n",
    "      precision_score(ground_truth, zero_shot_cot_label))\n",
    "print(\"Recall with TODO with CoT: \",\n",
    "      recall_score(ground_truth, zero_shot_cot_label))\n",
    "print(\"Accuracy with TODO with CoT: \",\n",
    "      accuracy_score(ground_truth, zero_shot_cot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_shot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'two_shot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m two_shot \u001B[38;5;241m=\u001B[39m \u001B[43mtwo_shot\u001B[49m\u001B[38;5;241m.\u001B[39mrename(columns\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2_shot_label\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[1;32m      2\u001B[0m two_shot[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2_shot_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m two_shot[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2_shot_label\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBIASED\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m two_shot[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2_shot_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m two_shot[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2_shot_label\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNOT BIASED\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'two_shot' is not defined"
     ]
    }
   ],
   "source": [
    "two_shot = two_shot.rename(columns={\"label\": \"2_shot_label\"})\n",
    "two_shot['2_shot_label'] = two_shot['2_shot_label'].replace('BIASED', 1)\n",
    "two_shot['2_shot_label'] = two_shot['2_shot_label'].replace('NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, two_shot[['text', '2_shot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "two_shot_label = df_merge['2_shot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (2 shot): \",\n",
    "      f1_score(ground_truth, two_shot_label))\n",
    "print(\"Precision with TODO with (2 shot): \",\n",
    "      precision_score(ground_truth, two_shot_label))\n",
    "print(\"Recall with TODO with (2 shot): \",\n",
    "      recall_score(ground_truth, two_shot_label))\n",
    "print(\"Accuracy with TODO with (2 shot): \",\n",
    "      accuracy_score(ground_truth, two_shot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "four_shot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "four_shot = four_shot.rename(columns={\"label\": \"4_shot_label\"})\n",
    "four_shot['4_shot_label'] = four_shot['4_shot_label'].replace('BIASED', 1)\n",
    "four_shot['4_shot_label'] = four_shot['4_shot_label'].replace('NOT BIASED', 0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, four_shot[['text', '4_shot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "four_shot_label = df_merge['4_shot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (4 shot): \",\n",
    "      f1_score(ground_truth, four_shot_label))\n",
    "print(\"Precision with TODO with (4 shot): \",\n",
    "      precision_score(ground_truth, four_shot_label))\n",
    "print(\"Recall with TODO with (4 shot): \",\n",
    "      recall_score(ground_truth, four_shot_label))\n",
    "print(\"Accuracy with TODO with (4 shot): \",\n",
    "      accuracy_score(ground_truth, four_shot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 8-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eight_shot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eight_shot = eight_shot.rename(columns={\"label\": \"8_shot_label\"})\n",
    "eight_shot['8_shot_label'] = eight_shot['8_shot_label'].replace('BIASED', 1)\n",
    "eight_shot['8_shot_label'] = eight_shot['8_shot_label'].replace('NOT BIASED',\n",
    "                                                                0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, eight_shot[['text', '8_shot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "eight_shot_label = df_merge['8_shot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (8 shot): \",\n",
    "      f1_score(ground_truth, eight_shot_label))\n",
    "print(\"Precision with TODO with (8 shot): \",\n",
    "      precision_score(ground_truth, eight_shot_label))\n",
    "print(\"Recall with TODO with (8 shot): \",\n",
    "      recall_score(ground_truth, eight_shot_label))\n",
    "print(\"Accuracy with TODO with (8 shot): \",\n",
    "      accuracy_score(ground_truth, eight_shot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_shot_cot = two_shot_cot.rename(columns={\"label\": \"2_shot_cot_label\"})\n",
    "two_shot_cot['2_shot_cot_label'] = two_shot_cot['2_shot_cot_label'].replace('BIASED', 1)\n",
    "two_shot_cot['2_shot_cot_label'] = two_shot_cot['2_shot_cot_label'].replace('NOT BIASED',\n",
    "                                                                0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, two_shot_cot[['text', '2_shot_cot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "two_shot_cot_label = df_merge['2_shot_cot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (2 shot CoT): \",\n",
    "      f1_score(ground_truth, two_shot_cot_label))\n",
    "print(\"Precision with TODO with (2 shot CoT): \",\n",
    "      precision_score(ground_truth, two_shot_cot_label))\n",
    "print(\"Recall with TODO with (2 shot CoT): \",\n",
    "      recall_score(ground_truth, two_shot_cot_label))\n",
    "print(\"Accuracy with TODO with (2 shot CoT): \",\n",
    "      accuracy_score(ground_truth, two_shot_cot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "four_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "four_shot_cot = four_shot_cot.rename(columns={\"label\": \"4_shot_cot_label\"})\n",
    "four_shot_cot['4_shot_cot_label'] = four_shot_cot['4_shot_cot_label'].replace('BIASED', 1)\n",
    "four_shot_cot['4_shot_cot_label'] = four_shot_cot['4_shot_cot_label'].replace('NOT BIASED',\n",
    "                                                                0)\n",
    "\n",
    "df_merge = pd.merge(df_merge, four_shot_cot[['text', '4_shot_cot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "four_shot_cot_label = df_merge['4_shot_cot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1-Score with TODO with (4 shot CoT): \",\n",
    "      f1_score(ground_truth, four_shot_cot_label))\n",
    "print(\"Precision with TODO with (4 shot CoT): \",\n",
    "      precision_score(ground_truth, four_shot_cot_label))\n",
    "print(\"Recall with TODO with (4 shot CoT): \",\n",
    "      recall_score(ground_truth, four_shot_cot_label))\n",
    "print(\"Accuracy with TODO with (4 shot CoT): \",\n",
    "      accuracy_score(ground_truth, four_shot_cot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 8-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>response</th>\n",
       "      <th>raw_data</th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But instead of closing loopholes that allow cr...</td>\n",
       "      <td>The sentence is biased because it uses emotion...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"You think I'm joking,\" he continued. \"</td>\n",
       "      <td>The sentence is a direct quote from an individ...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes, the rich are rich and want to stay that w...</td>\n",
       "      <td>The sentence is biased because it presents a n...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Americans are carrying $1.57 trillion in outst...</td>\n",
       "      <td>The sentence presents a factual statement abou...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republican state legislators from Texas to Ari...</td>\n",
       "      <td>The sentence is biased because it employs emot...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>Far-right talk show host Glenn Beck is being s...</td>\n",
       "      <td>The sentence is biased because it uses strong,...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>After President Donald Trump’s news conference...</td>\n",
       "      <td>The sentence is biased because it accuses the ...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>When Republicans warned Democrats that impeach...</td>\n",
       "      <td>The sentence is biased because it uses emotion...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>Sen. Tom Cotton (R-AR) says it “makes absolute...</td>\n",
       "      <td>The sentence is biased because it uses loaded ...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>The march then continued to the United States ...</td>\n",
       "      <td>The sentence presents a neutral account of eve...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>You are an expert in media bias.\\n\\nInstructio...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3214 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     But instead of closing loopholes that allow cr...   \n",
       "1               \"You think I'm joking,\" he continued. \"   \n",
       "2     Yes, the rich are rich and want to stay that w...   \n",
       "3     Americans are carrying $1.57 trillion in outst...   \n",
       "4     Republican state legislators from Texas to Ari...   \n",
       "...                                                 ...   \n",
       "4016  Far-right talk show host Glenn Beck is being s...   \n",
       "4017  After President Donald Trump’s news conference...   \n",
       "4018  When Republicans warned Democrats that impeach...   \n",
       "4019  Sen. Tom Cotton (R-AR) says it “makes absolute...   \n",
       "4020  The march then continued to the United States ...   \n",
       "\n",
       "                                               response  \\\n",
       "0     The sentence is biased because it uses emotion...   \n",
       "1     The sentence is a direct quote from an individ...   \n",
       "2     The sentence is biased because it presents a n...   \n",
       "3     The sentence presents a factual statement abou...   \n",
       "4     The sentence is biased because it employs emot...   \n",
       "...                                                 ...   \n",
       "4016  The sentence is biased because it uses strong,...   \n",
       "4017  The sentence is biased because it accuses the ...   \n",
       "4018  The sentence is biased because it uses emotion...   \n",
       "4019  The sentence is biased because it uses loaded ...   \n",
       "4020  The sentence presents a neutral account of eve...   \n",
       "\n",
       "                                               raw_data  \\\n",
       "0     You are an expert in media bias.\\n\\nInstructio...   \n",
       "1     You are an expert in media bias.\\n\\nInstructio...   \n",
       "2     You are an expert in media bias.\\n\\nInstructio...   \n",
       "3     You are an expert in media bias.\\n\\nInstructio...   \n",
       "4     You are an expert in media bias.\\n\\nInstructio...   \n",
       "...                                                 ...   \n",
       "4016  You are an expert in media bias.\\n\\nInstructio...   \n",
       "4017  You are an expert in media bias.\\n\\nInstructio...   \n",
       "4018  You are an expert in media bias.\\n\\nInstructio...   \n",
       "4019  You are an expert in media bias.\\n\\nInstructio...   \n",
       "4020  You are an expert in media bias.\\n\\nInstructio...   \n",
       "\n",
       "                                                  query label  \n",
       "0     You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "1     You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "2     You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "3     You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "4     You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "...                                                 ...   ...  \n",
       "4016  You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "4017  You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "4018  You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "4019  You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "4020  You are an expert in media bias.\\n\\nInstructio...     ?  \n",
       "\n",
       "[3214 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eight_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>response</th>\n",
       "      <th>raw_data</th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, response, raw_data, query, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "def update_label(row):\n",
    "    if row['response'].startswith('BIASED') and row['label'] == '?':\n",
    "        return 'BIASED'\n",
    "    elif row['response'].startswith('NOT BIASED') and row[\n",
    "        'label'] == '?':\n",
    "        return 'NOT BIASED'\n",
    "    elif row['response'].startswith('Classification: NOT BIASED') and row[\n",
    "        'label'] == '?':\n",
    "        return 'NOT BIASED'\n",
    "    elif row['response'].startswith('Classification: BIASED') and row[\n",
    "        'label'] == '?':\n",
    "        return 'BIASED'\n",
    "    elif row['response'].startswith('Classify the sentence above as BIASED or NOT BIASED.\\n\\nOutput: NOT BIASED') and row[\n",
    "        'label'] == '?':\n",
    "        return 'NOT BIASED'\n",
    "    elif row['response'].startswith('Classify the sentence above as BIASED or NOT BIASED.\\n\\nOutput: BIASED') and row[\n",
    "        'label'] == '?':\n",
    "        return 'BIASED'\n",
    "    elif find_first_occurrence(row['response']) == \"The answer is BIASED\"and row[\n",
    "        'label'] == '?':\n",
    "        return 'BIASED'\n",
    "    elif find_first_occurrence(row['response']) == \"The answer is NOT BIASED\"and row[\n",
    "        'label'] == '?':\n",
    "        return 'NOT BIASED'\n",
    "    else:\n",
    "        return row['label']\n",
    "\n",
    "\n",
    "eight_shot_cot['label'] = eight_shot_cot.apply(update_label, axis=1)\n",
    "eight_shot_cot.query(\"label == '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eight_shot_cot = eight_shot_cot.rename(columns={\"label\": \"8_shot_cot_label\"})\n",
    "eight_shot_cot['8_shot_cot_label'] = eight_shot_cot['8_shot_cot_label'].replace('BIASED', 1)\n",
    "eight_shot_cot['8_shot_cot_label'] = eight_shot_cot['8_shot_cot_label'].replace('NOT BIASED',\n",
    "                                                                0)\n",
    "                                                                \n",
    "df_merge = pd.merge(df_babe, eight_shot_cot[['text', '8_shot_cot_label']], on='text')\n",
    "ground_truth = df_merge['label']\n",
    "eight_shot_cot_label = df_merge['8_shot_cot_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score with TODO with (8 shot CoT):  0.7694425326909842\n",
      "Precision with TODO with (8 shot CoT):  0.7081925675675675\n",
      "Recall with TODO with (8 shot CoT):  0.8422903063787042\n",
      "Accuracy with TODO with (8 shot CoT):  0.7500621735886596\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-Score with TODO with (8 shot CoT): \",\n",
    "      f1_score(ground_truth, eight_shot_cot_label))\n",
    "print(\"Precision with TODO with (8 shot CoT): \",\n",
    "      precision_score(ground_truth, eight_shot_cot_label))\n",
    "print(\"Recall with TODO with (8 shot CoT): \",\n",
    "      recall_score(ground_truth, eight_shot_cot_label))\n",
    "print(\"Accuracy with TODO with (8 shot CoT): \",\n",
    "      accuracy_score(ground_truth, eight_shot_cot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Comparison and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(ax, df, true_labels_column, predicted_labels_column,\n",
    "                          title=None\n",
    "                          ):\n",
    "    predicted_labels = df[f'{predicted_labels_column}']\n",
    "    true_labels = df[f'{true_labels_column}']\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "\n",
    "    # Display confusion matrix heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True,\n",
    "                yticklabels=True, ax=ax)\n",
    "\n",
    "    title = title if title else predicted_labels_column\n",
    "\n",
    "    ax.set_title(f'Confusion Matrix - {title}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices')\n",
    "\n",
    "# Plot each confusion matrix\n",
    "plot_confusion_matrix(axes[0, 0], df_merge, 'label', '0_shot_label', '0_shot')\n",
    "plot_confusion_matrix(axes[0, 1], df_merge, 'label',\n",
    "                      '0_shot_with_system_label', '0_shot_with_system')\n",
    "plot_confusion_matrix(axes[0, 2], df_merge, 'label', '0_shot_cot_label',\n",
    "                      '0_shot_cot')\n",
    "plot_confusion_matrix(axes[1, 0], df_merge, 'label', '2_shot_label', '2_shot')\n",
    "plot_confusion_matrix(axes[1, 1], df_merge, 'label', '4_shot_label', '4_shot')\n",
    "plot_confusion_matrix(axes[1, 2], df_merge, 'label', '8_shot_label', '8_shot')\n",
    "\n",
    "plt.tight_layout(\n",
    "    rect=[0, 0, 1, 0.96])  # Adjust layout to prevent title overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Krippendorff Alpha in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "\n",
    "runs = ['0_shot_label', '0_shot_with_system_label', '0_shot_cot_label',\n",
    "        '2_shot_label', '4_shot_label', '8_shot_label']\n",
    "\n",
    "\n",
    "def compute_krippendorff_alpha(df, predicted_columns):\n",
    "    pred_map = {}\n",
    "    for run in predicted_columns:\n",
    "        predicted_labels = df[run]\n",
    "        pred_map[run] = predicted_labels\n",
    "\n",
    "    # Check if there is variability in the ratings\n",
    "    unique_labels_counts = df[predicted_columns].nunique(axis=1)\n",
    "    if unique_labels_counts.max() == 1:\n",
    "        # All ratings are the same, return a special value or handle accordingly\n",
    "        return 0\n",
    "\n",
    "    reliability_data = df[predicted_columns].values.tolist()\n",
    "\n",
    "    # Calculate Krippendorff's alpha\n",
    "    alpha = krippendorff.alpha(reliability_data=list(pred_map.values()),\n",
    "                               level_of_measurement='nominal')\n",
    "\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_value = compute_krippendorff_alpha(df_merge, runs)\n",
    "print(f\"Krippendorff's Alpha (all runs): {alpha_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def compute_krippendorff_alpha_for_k_runs(df, runs, k=None):\n",
    "    # Initialize variables to store the best combination and alpha\n",
    "    if k is None:\n",
    "        k = len(runs)\n",
    "\n",
    "    best_combination = None\n",
    "    best_alpha = 0  # Assuming alpha ranges from 0 to 1\n",
    "\n",
    "    # Iterate through all possible combinations\n",
    "    for combination in itertools.combinations(runs, k):\n",
    "\n",
    "        alpha_value = compute_krippendorff_alpha(df, list(combination))\n",
    "\n",
    "        # Print alpha for the current combination\n",
    "        print(f\"Combination: {combination}, Alpha: {alpha_value}\")\n",
    "\n",
    "        # Update best combination and alpha if a higher alpha is found\n",
    "        if alpha_value > best_alpha:\n",
    "            best_alpha = alpha_value\n",
    "            best_combination = combination\n",
    "\n",
    "    # Print the best combination and alpha\n",
    "    print(f\"\\nBest Combination: {best_combination}, Best Alpha: {best_alpha}\")\n",
    "    return best_alpha, best_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_krippendorff_alpha_for_k_runs(df_merge, runs, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a80f9e0de999ee68a5c4d727299d170641bcafce6104c26cbe6897ebd010016"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
