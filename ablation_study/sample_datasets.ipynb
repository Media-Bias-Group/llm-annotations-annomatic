{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unbalanced pool of all viable sentences (not downsampled)\n",
    "d=pd.read_parquet('../sentence_pool/data/output/final_sentence_pool_unbalanced.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subsets with only specific party and for left also with speciifc source\n",
    "left = d[(d['source_party']=='Left') | (d['source_party']=='Lean Left')]\n",
    "right = d[(d['source_party']=='Right') | (d['source_party']=='Lean Right')]\n",
    "center = d[d['source_party']=='Center']\n",
    "alternet = d[d['source_name']=='alternet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the annomatic dev/test to prevent data leakage\n",
    "dev = pd.read_parquet('../annomatic-dataset/data/training/anno-lexical-dev.parquet')\n",
    "test = pd.read_parquet('../annomatic-dataset/data/training/anno-lexical-test.parquet')\n",
    "dev_test = pd.concat([dev,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sentences from dataframes that are in the annolexical dev/test set\n",
    "left = left[~left['text'].isin(dev_test['text'])]\n",
    "right = right[~right['text'].isin(dev_test['text'])]\n",
    "center = center[~center['text'].isin(dev_test['text'])]\n",
    "alternet = alternet[~alternet['text'].isin(dev_test['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read annotations to prevent redundant annotations\n",
    "annotated=pd.read_parquet('../annomatic-dataset/data/output/final_sentence_pool_annotated.parquet')\n",
    "annotated=annotated[['text','label_zephyr','label_openchat','label_llama','final_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the already annotated sentences\n",
    "\n",
    "right = pd.merge(right, annotated, on='text', how='left')\n",
    "left = pd.merge(left, annotated, on='text', how='left')\n",
    "center = pd.merge(center, annotated, on='text', how='left')\n",
    "alternet = pd.merge(alternet, annotated, on='text', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into annotated and yet to be annotated and save\n",
    "\n",
    "right_annotated = right[~right.final_label.isna()]\n",
    "right2annotate = right[right.final_label.isna()]\n",
    "\n",
    "right_annotated.to_parquet('data/right/right_train_annotated.parquet')\n",
    "right2annotate[['text','source_party','source_name']].to_parquet('data/right/right_train_to_annotate.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_annotated = left[~left.final_label.isna()]\n",
    "left2annotate = left[left.final_label.isna()]\n",
    "\n",
    "left2annotate_b = left2annotate[left2annotate.bias_estimate==1].sample(30000)\n",
    "left2annotate_n = left2annotate[left2annotate.bias_estimate==0].sample(len(left2annotate_b))\n",
    "\n",
    "left2annotate = pd.concat([left2annotate_b,left2annotate_n]).sample(frac=1)\n",
    "\n",
    "left_annotated.to_parquet('data/left/left_train_annotated.parquet')\n",
    "left2annotate[['text','source_party','source_name']].to_parquet('data/left/left_train_to_annotate.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_annotated = center[~center.final_label.isna()]\n",
    "center2annotate = center[center.final_label.isna()]\n",
    "\n",
    "center_annotated.to_parquet('data/center/center_train_annotated.parquet')\n",
    "center2annotate[['text','source_party','source_name']].to_parquet('data/center/center_train_to_annotate.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternet_annotated = alternet[~alternet.final_label.isna()]\n",
    "alternet2annotate = alternet[alternet.final_label.isna()]\n",
    "\n",
    "alternet_annotated.to_parquet('data/alternet/alternet_train_annotated.parquet')\n",
    "alternet2annotate[['text','source_party','source_name']].to_parquet('data/alternet/alternet_train_to_annotate.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
